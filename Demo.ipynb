{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c6ce0d-7f6d-45b8-a571-e13db4134b0c",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "[\"Recipe Reviews and User Feedback Dataset\"](https://archive.ics.uci.edu/dataset/911/recipe+reviews+and+user+feedback+dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ec6b4f45-d93d-4258-95f0-ff56c8fe5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Cowardly refusing to download \"recipe+reviews+and+user+feedback+dataset.zip\" as it already exists...\n"
     ]
    }
   ],
   "source": [
    "def download_recipe_data(rootpath):\n",
    "    import urllib.request\n",
    "    import os\n",
    "    rootpath = os.path.abspath(rootpath)\n",
    "    os.makedirs(rootpath, exist_ok=True)\n",
    "        \n",
    "    url = 'https://archive.ics.uci.edu/static/public/911/recipe+reviews+and+user+feedback+dataset.zip'\n",
    "    filename = url.rsplit('/', 1)[1]\n",
    "    filepath = os.path.join(rootpath, filename)\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        print(f'===> Cowardly refusing to download \"{filename}\" as it already exists...')\n",
    "        return filepath\n",
    "    urllib.request.urlretrieve(url, filepath)\n",
    "    return filepath\n",
    "\n",
    "def unzip_recipe_data(zippath):\n",
    "    import zipfile\n",
    "    extractpath = os.path.abspath(os.path.dirname(zippath))\n",
    "    with zipfile.ZipFile(zippath, 'r') as zipf:\n",
    "        zipf.extractall(extractpath)\n",
    "    # Expecting a single CSV file + a single ZIP file\n",
    "    files = os.listdir(extractpath)\n",
    "    assert len(files) == 2\n",
    "    filename = files[0] if files[0].endswith('csv') else files[1]\n",
    "    return os.path.join(extractpath, filename)\n",
    "\n",
    "root_path = './data/'\n",
    "csv_path = unzip_recipe_data(download_recipe_data(root_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "ba0af24a-414f-4f6c-b0a1-328ee9edb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "83b6630b-f370-42e1-8b0a-0e6d69fe2b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipe_name</th>\n",
       "      <th>user_reputation</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>thumbs_up</th>\n",
       "      <th>thumbs_down</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Creamy White Chili</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>I tweaked it a little, removed onions because ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy White Chili</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Bush used to have a white chili bean and it ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creamy White Chili</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>I have a very complicated white chicken chili ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creamy White Chili</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In your introduction, you mentioned cream chee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Creamy White Chili</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wonderful! I made this for a &amp;#34;Chili/Stew&amp;#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18177</th>\n",
       "      <td>Mamaw Emily’s Strawberry Cake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>This Strawberry Cake has been a family favorit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18178</th>\n",
       "      <td>Mamaw Emily’s Strawberry Cake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;p&gt;I received endless compliments on this cake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18179</th>\n",
       "      <td>Mamaw Emily’s Strawberry Cake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>This cake was delicious and so moist! I didn&amp;#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18180</th>\n",
       "      <td>Mamaw Emily’s Strawberry Cake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>I just made this too.  It is wonderful.  As fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18181</th>\n",
       "      <td>Mamaw Emily’s Strawberry Cake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>OMG, you must make this cake. I made it for my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18182 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         recipe_name  user_reputation  reply_count  thumbs_up  \\\n",
       "0                 Creamy White Chili                1            0          0   \n",
       "1                 Creamy White Chili               50            0          7   \n",
       "2                 Creamy White Chili               10            0          3   \n",
       "3                 Creamy White Chili                1            2          2   \n",
       "4                 Creamy White Chili               10            1          7   \n",
       "...                              ...              ...          ...        ...   \n",
       "18177  Mamaw Emily’s Strawberry Cake                1            0          0   \n",
       "18178  Mamaw Emily’s Strawberry Cake                1            0          0   \n",
       "18179  Mamaw Emily’s Strawberry Cake                1            0          0   \n",
       "18180  Mamaw Emily’s Strawberry Cake                1            0          0   \n",
       "18181  Mamaw Emily’s Strawberry Cake                1            0          1   \n",
       "\n",
       "       thumbs_down  stars                                               text  \n",
       "0                0      5  I tweaked it a little, removed onions because ...  \n",
       "1                0      5  Bush used to have a white chili bean and it ma...  \n",
       "2                0      5  I have a very complicated white chicken chili ...  \n",
       "3                0      0  In your introduction, you mentioned cream chee...  \n",
       "4                0      0  Wonderful! I made this for a &#34;Chili/Stew&#...  \n",
       "...            ...    ...                                                ...  \n",
       "18177            0      5  This Strawberry Cake has been a family favorit...  \n",
       "18178            0      5  <p>I received endless compliments on this cake...  \n",
       "18179            0      5  This cake was delicious and so moist! I didn&#...  \n",
       "18180            0      5  I just made this too.  It is wonderful.  As fo...  \n",
       "18181            0      5  OMG, you must make this cake. I made it for my...  \n",
       "\n",
       "[18182 rows x 7 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df = df.drop(['user_id', 'user_name', 'Unnamed: 0', 'recipe_code', 'comment_id', 'recipe_number', 'created_at', 'best_score'], axis='columns')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7432c8-e178-4916-afe7-4fa8ad158633",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "f1e185c1-d9dc-4b28-9d10-e588abbd3a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def process_text(txt):\n",
    "    r'''\n",
    "    Transformations\n",
    "        1. Lowercase\n",
    "        2. remove all non-digit and non-letter characters\n",
    "    '''\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9]+')\n",
    "    f1 = lambda s: pattern.sub(' ', s)\n",
    "    f2 = lambda s: s.strip()\n",
    "    f3 = lambda s: f'<{s}>'\n",
    "    if isinstance(txt, pd.Series):\n",
    "        return txt.fillna('').str\\\n",
    "                  .lower()\\\n",
    "                  .apply(f1)\\\n",
    "                  .apply(f2)\n",
    "    elif isinstance(txt, (list, tuple, np.ndarray)):\n",
    "        return np.vectorize(process_txt)(txt)\n",
    "    elif isinstance(txt, str):\n",
    "        txt = txt.lower()\n",
    "        txt = f1(txt)\n",
    "        txt = f2(txt)\n",
    "        return txt\n",
    "    else:\n",
    "        raise ValueError(f'Cannot process type {type(txt)}...')\n",
    "\n",
    "def pad_and_batch(tokens, tokenizer, batch_size=32):\n",
    "    tokens.sort(key=len, reverse=True)\n",
    "    batches = []\n",
    "    for idx in range(0, len(tokens), batch_size):\n",
    "        batch = data[idx:idx+batch_size]\n",
    "        max_length = len(batch[0])\n",
    "        padded_batch = [sample + vocabulary.pad_ * (max_length - len(sample) for sample in batch]\n",
    "        batches.append(torch.tensor(padded_batch))\n",
    "    return batches        \n",
    "\n",
    "class Tokenizer:\n",
    "    r'''Simple tokenization scheme that learns the vocabulary from the text.\n",
    "    '''\n",
    "    def __init__(self, word_len, overlap, ):\n",
    "        self.word_len = word_len\n",
    "        self.overlap = overlap\n",
    "        \n",
    "        self.unk = '<UNK>'\n",
    "        self.pad = '<PAD>'\n",
    "        self.start = '<START>'\n",
    "        self.end = '<END>'\n",
    "        \n",
    "        self.tokens = [self.unk, self.pad, self.start, self.end]\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(self.tokens)}  # word -> index\n",
    "\n",
    "        # Indices\n",
    "        self.unk_ = self.tokens.index(self.unk)\n",
    "        self.pad_ = self.tokens.index(self.pad)\n",
    "        self.start_ = self.tokens.index(self.start)\n",
    "        self.end_ = self.tokens.index(self.end)\n",
    "\n",
    "    def split(self, txt: str):\n",
    "        return [txt[idx:idx+self.word_len] for idx in range(1, len(txt)-self.word_len+1, self.word_len - self.overlap)]\n",
    "\n",
    "    def fit(self, data):\n",
    "        data = process_text(data)\n",
    "        if isinstance(data, (list, tuple, np.ndarray, pd.Series)):\n",
    "            np.vectorize(self.fit)(data)\n",
    "        elif isinstance(data, str):\n",
    "            tokens = self.split(data)\n",
    "            tokens = set(tokens)\n",
    "            tokens -= self.vocabulary.keys()\n",
    "            offset = len(self.tokens)\n",
    "            vocabulary = {s: idx+offset for idx, s in enumerate(tokens)}\n",
    "            self.vocabulary.update(vocabulary)\n",
    "            self.tokens += tokens            \n",
    "        else:\n",
    "            raise ValueError(f'Cannot process \"{type(data)}\"...')\n",
    "\n",
    "    def __call__(self, txt):\n",
    "        txt = process_text(txt)\n",
    "        if isinstance(txt, (list, tuple)):\n",
    "            txt = [self(t) for t in txt]\n",
    "            return txt\n",
    "        elif isinstance(txt, (np.ndarray, pd.Series)):\n",
    "            txt = np.array([self(t) for t in txt], dtype=object)\n",
    "            return txt\n",
    "        elif isinstance(txt, str):\n",
    "            tokens = self.split(txt)\n",
    "            tokens = [self.start_] + [self.vocabulary.get(tok, self.unk_) for tok in tokens] + [self.end_]\n",
    "            return tokens\n",
    "        else:\n",
    "            raise ValueError(f'Unknown type \"{type(txt)}\"...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if isinstance(i, int):\n",
    "            return self.tokens[i]\n",
    "        else:\n",
    "            return self.vocabulary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5e708c-ade9-4b9b-a3d2-9a97846cf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r'''Positional encoding'''\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, nfeatures, feature_embed_size, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        self.text_encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.feature_encoders = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings, feature_embed_size) for num_embeddings in nfeatures\n",
    "        ])\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(ninp + feature_embed_size * len(nfeatures), nhead, nhid, dropout), nlayers)\n",
    "        self.ninp = ninp\n",
    "        self.feature_embed_size = feature_embed_size\n",
    "        self.decoder = nn.Linear(ninp + feature_embed_size * len(nfeatures), ntoken)\n",
    "\n",
    "    def forward(self, text, features):\n",
    "        text_embedded = self.text_encoder(text) * math.sqrt(self.ninp)\n",
    "        feature_embeddings = [encoder(features[:, i]) for i, encoder in enumerate(self.feature_encoders)]\n",
    "        feature_embeddings = torch.cat(feature_embeddings, dim=-1)\n",
    "\n",
    "        src = torch.cat((text_embedded, feature_embeddings), dim=-1)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "fb3538dc-bd9c-4b6c-b37a-a16ebdf5898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(word_len=4, overlap=2)\n",
    "tokenizer.fit(df['text'])\n",
    "# tokenizer(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3907dcaf-322c-46fd-9318-6983bb2d7e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_unique_values1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[359], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ntokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer) \u001b[38;5;66;03m# size of vocabulary for text\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nfeatures \u001b[38;5;241m=\u001b[39m [\u001b[43mnum_unique_values1\u001b[49m, num_unique_values2, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;66;03m# List of unique values count for each categorical feature\u001b[39;00m\n\u001b[1;32m      3\u001b[0m feature_embed_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m# Size of embeddings for each feature\u001b[39;00m\n\u001b[1;32m      4\u001b[0m emsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;66;03m# embedding dimension for text\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_unique_values1' is not defined"
     ]
    }
   ],
   "source": [
    "ntokens = len(tokenizer) # size of vocabulary for text\n",
    "nfeatures = [num_unique_values1, num_unique_values2, ...] # List of unique values count for each categorical feature\n",
    "feature_embed_size = 10 # Size of embeddings for each feature\n",
    "emsize = 200 # embedding dimension for text\n",
    "nhid = 200 # dimension of the feedforward network in nn.TransformerEncoder\n",
    "nlayers = 2 # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # number of heads in the multiheadattention models\n",
    "dropout = 0.2 # dropout value\n",
    "\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, nfeatures, feature_embed_size, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dc790e-0599-4736-9a02-c7369e3f0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assuming your dataset is a list of (text, features, labels) tuples\n",
    "train_dataset = ... # Your dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for text, features, labels in train_loader:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(text, features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "    # Validation step...\n",
    "    # Save model..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
